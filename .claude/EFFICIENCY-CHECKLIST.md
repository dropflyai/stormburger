# EFFICIENCY CHECKLIST

**Purpose:** Pre-response verification checklist. Read this BEFORE every response to prevent common mistakes.

**Last Updated:** 2025-12-11

---

## ğŸš¨ MANDATORY PRE-RESPONSE CHECKLIST

Run through this checklist BEFORE responding to every user request:

### 1. Have We Solved This Before?

- [ ] Search DEBUGGING-LOG.md for similar symptoms/errors
- [ ] Check tags: #authentication, #supabase, #vercel, #deployment, etc.
- [ ] Review "Related Issues" section for patterns
- [ ] If found: Use previous solution, don't reinvent

**Command to search:**
```bash
grep -i "keyword" .claude/DEBUGGING-LOG.md
```

**Common searches:**
- Error codes: "PGRST116", "404", "JWT"
- Technologies: "supabase", "vercel", "aws"
- Features: "authentication", "routing", "deployment"

---

### 2. Can This Be Automated?

**STOP and check AUTOMATION-PLAYBOOK.md if you're about to:**
- [ ] Ask user to manually run migrations
- [ ] Ask user to manually deploy to Vercel
- [ ] Ask user to copy/paste commands
- [ ] Ask user to manually SSH into servers
- [ ] Say "I can't automate X"

**Red flag phrases that trigger this check:**
- "Please manually..."
- "I can't automate..."
- "Copy and paste this..."
- "Run this in the dashboard..."
- "SSH into the server and..."

**Where to check:**
- AUTOMATION-PLAYBOOK.md â†’ Supabase Operations
- AUTOMATION-PLAYBOOK.md â†’ Vercel Deployments
- AUTOMATION-PLAYBOOK.md â†’ AWS EC2 Management
- scripts/ directory for existing scripts

---

### 3. Do We Have These Credentials?

**STOP and check credentials/ directory if you're about to:**
- [ ] Ask user "What's the [service] password/key/token?"
- [ ] Request API keys we've used before
- [ ] Ask for database credentials
- [ ] Request deployment tokens

**Check these locations IN ORDER:**
1. `credentials/.env` (primary storage)
2. `credentials/services/[service].env` (service-specific)
3. Backend project `.env` files (for reference)
4. DEBUGGING-LOG.md (search for "Added [SERVICE] credentials")

**Standard credentials we should have:**
- SUPABASE_DB_PASSWORD
- SUPABASE_URL, SUPABASE_ANON_KEY, SUPABASE_SERVICE_KEY
- VERCEL_TOKEN, VERCEL_ORG_ID, VERCEL_PROJECT_ID
- AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION
- GITHUB_TOKEN
- Third-party API keys (POLYGON_API_KEY, OPENAI_API_KEY, etc.)

---

### 4. Will I Verify Before Claiming Success?

**ğŸš¨ MANDATORY TRIPLE VERIFICATION PROTOCOL ğŸš¨**

**ABSOLUTE RULE:** NEVER claim "it's done" or "it's working" until ALL THREE verification levels pass.

**STOP if you're about to:**
- [ ] Say "it's working now" without testing
- [ ] Deploy without verification
- [ ] Claim "this should work" without proof
- [ ] Ask user to "check if there are any errors" (YOU check first!)
- [ ] Ask user to copy/paste error messages (YOU find them first!)

**Required: Run Triple Verification Script**

```bash
# For frontend/web applications
./scripts/automation/complete-verification.sh https://www.yourapp.com

# OR use Python version
python3 scripts/automation/triple-verify.py https://www.yourapp.com

# ONLY claim success if exit code = 0
# If exit code = 1, FIX ERRORS and re-run
```

**What Triple Verification Does:**

**Level 1: Automated Testing**
- Loads page with Playwright
- Captures all console messages (errors, warnings, logs)
- Detects JavaScript exceptions
- Records network failures (404s, 500s, etc.)
- Takes screenshots
- EXIT CRITERIA: Status 200, no console errors, no network failures

**Level 2: Visual Verification**
- Screenshots entire page
- Verifies page loads completely
- Checks page title
- Confirms URL is correct
- EXIT CRITERIA: Page renders without crashing

**Level 3: Error Scanning**
- Scans for console.error
- Scans for console.warn
- Checks for unhandled promise rejections
- Verifies no 404s on critical resources
- EXIT CRITERIA: Zero critical errors detected

**Only claim success when:**
```
âœ… Level 1 PASSED
âœ… Level 2 PASSED
âœ… Level 3 PASSED
```

**Full documentation:** `.claude/TRIPLE-VERIFICATION-PROTOCOL.md`

---

### 5. Have I Mapped the User Journey?

**STOP if you're about to build authentication/user-facing features without:**
- [ ] Identifying WHO the user is
- [ ] Understanding WHERE they're coming from
- [ ] Defining WHAT they expect to see
- [ ] Explaining WHY they'd take next action
- [ ] Planning HOW the system facilitates this

**User Journey Template:**
```
1. WHO: [New visitor | Returning user | Authenticated user]
2. WHERE: [Direct link | Search | Internal navigation]
3. WHAT: [Expected first screen/action]
4. WHY: [Motivation for next step]
5. HOW: [System design to facilitate]

Flow:
[Start] â†’ [Step 1] â†’ [Step 2] â†’ [Goal]
          â†“
       [Alternative path if condition X]
```

**Example (Authentication):**
```
1. WHO: New visitor
2. WHERE: Direct link to site root
3. WHAT: Expects to see login/signup page
4. WHY: Wants to access platform features
5. HOW: Redirect / â†’ /login.html

Flow:
/ â†’ /login â†’ /signup â†’ /onboarding â†’ /dashboard
     â†“
  (if authenticated, skip to /dashboard)
```

**Where to check:** SENIOR-ENGINEER-PRINCIPLES.md â†’ User Journey Mapping

---

### 6. Have I Checked the Schema?

**STOP if you're about to write SQL INSERT/UPDATE without:**
- [ ] Querying information_schema to see actual columns
- [ ] Verifying data types match
- [ ] Confirming constraints (NOT NULL, UNIQUE, etc.)

**Required schema check:**
```sql
SELECT column_name, data_type, is_nullable
FROM information_schema.columns
WHERE table_schema = 'public'
AND table_name = 'YOUR_TABLE'
ORDER BY ordinal_position;
```

**Then write query using ONLY confirmed columns.**

---

### 7. Will I Document the Solution?

**If solving a NEW issue, commit to:**
- [ ] Document in DEBUGGING-LOG.md using template
- [ ] Update AUTOMATION-PLAYBOOK.md if automation discovered
- [ ] Update COMMON-MISTAKES.md if pattern identified
- [ ] Cross-reference related issues

**When to document:**
- Immediately after solving issue
- Before moving to next task
- While details are fresh

**Use template from:** DEBUGGING-LOG.md â†’ Issue Template

---

### 8. Is There an Existing Script?

**STOP and check scripts/ directory if you're about to:**
- [ ] Write one-off commands
- [ ] Repeat similar commands from before
- [ ] Run deployment/migration/test manually

**Standard scripts directory:**
```
scripts/
â”œâ”€â”€ automation/
â”‚   â”œâ”€â”€ test-deployment.py
â”‚   â”œâ”€â”€ verify-build.sh
â”‚   â””â”€â”€ health-check.sh
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ deploy-to-vercel.sh
â”‚   â”œâ”€â”€ deploy-to-ec2.sh
â”‚   â””â”€â”€ deploy-ios.sh
â””â”€â”€ database/
    â”œâ”€â”€ run-migration.sh
    â”œâ”€â”€ backup-db.sh
    â””â”€â”€ seed-db.sh
```

**If script doesn't exist:**
1. Create reusable script
2. Make executable: `chmod +x script.sh`
3. Document in AUTOMATION-PLAYBOOK.md
4. Use script instead of one-off command

---

## ğŸ¯ Quick Decision Tree

Use this flowchart for every user request:

```
User Request Received
       â†“
[1] Have we solved this? â†’ YES â†’ Use previous solution
       â†“ NO                            â†“
[2] Can this be automated? â†’ YES â†’ Check AUTOMATION-PLAYBOOK
       â†“ NO                             â†“
[3] Need credentials? â†’ YES â†’ Check credentials/ directory
       â†“ NO                             â†“
[4] Building user feature? â†’ YES â†’ Map user journey first
       â†“ NO                             â†“
[5] Writing SQL? â†’ YES â†’ Check schema first
       â†“ NO                             â†“
[6] Existing script? â†’ YES â†’ Use script from scripts/
       â†“ NO                             â†“
Implement Solution
       â†“
[7] Verify with tests â†’ Must pass before claiming success
       â†“
[8] Document solution â†’ Update logs/playbook/mistakes
       â†“
Respond to User (with evidence)
```

---

## ğŸ“‹ Per-Task Checklists

### For Database Migrations

- [ ] Check credentials/.env for DB password
- [ ] Check schema with information_schema query
- [ ] Write migration targeting only existing columns
- [ ] Use scripts/database/run-migration.sh
- [ ] Verify with SELECT query after migration
- [ ] Document in DEBUGGING-LOG if new pattern

### For Vercel Deployments

- [ ] Check credentials/.env for VERCEL_TOKEN
- [ ] Use scripts/deployment/deploy-to-vercel.sh
- [ ] Write Playwright verification test
- [ ] Run test and capture output/screenshot
- [ ] Check for 404s on key routes
- [ ] Only claim success if tests pass

### For Authentication Features

- [ ] Map user journey from first principles
- [ ] Define all user states (new/returning/authenticated)
- [ ] Plan redirect logic before implementation
- [ ] Test as new user (clear browser state)
- [ ] Verify with Playwright script
- [ ] Document flow in README or user journey doc

### For API Integrations

- [ ] Check credentials/.env for API keys
- [ ] Test API with curl first
- [ ] Verify response structure
- [ ] Handle errors gracefully
- [ ] Write health check script
- [ ] Document API usage in AUTOMATION-PLAYBOOK

### For New Features

- [ ] Check DEBUGGING-LOG for similar implementations
- [ ] Map user journey if user-facing
- [ ] Check for existing scripts/utilities
- [ ] Write tests before claiming complete
- [ ] Verify tests pass
- [ ] Document any new patterns

---

## ğŸš« Red Flags Requiring Immediate Stop

If you catch yourself using these phrases, STOP and check this list:

| Red Flag Phrase | What to Check | Where to Look |
|----------------|---------------|---------------|
| "I can't automate..." | Can it actually be automated? | AUTOMATION-PLAYBOOK.md |
| "Please manually..." | Is there a script for this? | scripts/ directory |
| "It should work now" | Have you verified? | This checklist #4 |
| "What's the password for..." | Do we have it already? | credentials/.env |
| "Copy and paste this..." | Can we run it directly? | AUTOMATION-PLAYBOOK.md |
| "Let me deploy this" | Have you planned verification? | This checklist #4 |
| "I'll create this feature" | Have you mapped user journey? | This checklist #5 |
| "Here's the SQL..." | Have you checked schema? | This checklist #6 |

---

## ğŸ“Š Efficiency Scoring

After each task, score yourself:

**10/10 Efficiency:**
- [ ] Checked all relevant sections of checklist
- [ ] Found and reused previous solution OR documented new one
- [ ] Automated everything that could be automated
- [ ] Verified before claiming success
- [ ] Documented solution for future reference

**7-9/10 Efficiency:**
- [ ] Checked most sections of checklist
- [ ] Used some automation but missed opportunities
- [ ] Verified major functionality
- [ ] Documented most important parts

**4-6/10 Efficiency:**
- [ ] Checked some sections
- [ ] Mixed automation and manual steps
- [ ] Partial verification
- [ ] Incomplete documentation

**1-3/10 Efficiency:**
- [ ] Skipped checklist
- [ ] Asked for manual work that could be automated
- [ ] No verification
- [ ] No documentation

**0/10 Efficiency:**
- [ ] Claimed success without verification
- [ ] Asked for credentials we already have
- [ ] Repeated previous mistakes from DEBUGGING-LOG
- [ ] No documentation

**Target:** Consistent 10/10 scores

---

## ğŸ”„ Weekly Review

Every week, review efficiency:

**Monday Morning:**
- [ ] Read COMMON-MISTAKES.md
- [ ] Scan last week's DEBUGGING-LOG entries
- [ ] Review credentials/ for new additions
- [ ] Check scripts/ for new utilities

**Friday Afternoon:**
- [ ] Calculate average efficiency score for week
- [ ] Identify patterns in mistakes
- [ ] Update COMMON-MISTAKES.md if needed
- [ ] Plan improvements for next week

---

## âœ… Success Metrics

Track these to measure improvement:

| Metric | Target | How to Measure |
|--------|--------|----------------|
| False "can't automate" claims | 0 | Count per week |
| Unverified success claims | 0 | Count per week |
| Requests for existing credentials | 0 | Count per week |
| Repeated mistakes from log | 0 | Count per month |
| Issues documented within 1 hour | 100% | Log timestamps |
| Tests written before claiming done | 100% | Count per task |
| Average efficiency score | 10/10 | Weekly average |

---

## ğŸ“ Learning Mode

When encountering a new technology or pattern:

**Before using:**
- [ ] Search DEBUGGING-LOG for previous usage
- [ ] Check AUTOMATION-PLAYBOOK for patterns
- [ ] Read official documentation
- [ ] Test in isolation first

**After using:**
- [ ] Document in AUTOMATION-PLAYBOOK
- [ ] Create reusable script if applicable
- [ ] Note gotchas in COMMON-MISTAKES
- [ ] Add to verification checklist

---

## ğŸ’¡ Remember

**The goal is NOT speed, it's EFFICIENCY:**
- Taking 2 minutes to check this list â†’ Saves 30 minutes of rework
- Verifying before claiming success â†’ Prevents user frustration
- Documenting solutions â†’ Prevents solving same issue twice
- Using automation â†’ Scales better than manual work

**User's feedback:**
> "as of right now i would give you a 2 out of 10 for effiency. how do we get that to 10/10"

**This checklist is how we get to 10/10.**

---

## ğŸ“± Quick Reference Card

Print this and keep visible:

```
BEFORE EVERY RESPONSE:
â˜ Check DEBUGGING-LOG (have we solved this?)
â˜ Check AUTOMATION-PLAYBOOK (can we automate?)
â˜ Check credentials/ (do we have keys?)
â˜ Plan verification (how will I test?)

BEFORE CLAIMING SUCCESS:
â˜ Write test
â˜ Run test
â˜ Capture evidence
â˜ Tests pass?

AFTER SOLVING:
â˜ Document in DEBUGGING-LOG
â˜ Update AUTOMATION-PLAYBOOK if new automation
â˜ Update COMMON-MISTAKES if new pattern
â˜ Cross-reference related issues

RED FLAGS:
"I can't automate..." â†’ Check playbook
"Please manually..." â†’ Check scripts
"It should work..." â†’ Run verification
"What's the password..." â†’ Check credentials
```
